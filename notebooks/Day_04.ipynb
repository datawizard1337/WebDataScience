{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Web scraping** is used to extract (scrape) data from webpages on the Internet. The program that performs this task is usually called a **web scraper** or a **bot**. \n",
    "\n",
    "**Web crawling** is the process of exploring and oftentimes indexing the webpages on the Internet by following hyperlinks from webpage to webpage. The program that performs this task is usually called a **spider** or **web crawler**.\n",
    "\n",
    "Oftentimes, web scraping and web crawling are combined into a single program. I will continue using \"web scraping\" to denote both approaches.\n",
    "\n",
    "Web scraping can be used for both **focus crawls** which concentrate on crawling and scraping a single website (e.g. amazon.com) or **broad crawl** which does the same on many different websites.\n",
    "\n",
    "Common **use cases** for web scraping are:\n",
    "- search engines\n",
    "- price monitoring\n",
    "- content aggregators\n",
    "- collecting massive amounts of text data for the training of language models\n",
    "- copying online databases\n",
    "- research data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HyperText Markup Language (HTML) is the standard markup language for documents designed to be displayed in a web browser. Web browsers receive HTML documents from a web server and render the documents into multimedia web pages.\n",
    "\n",
    "This is an example for a simple html document:\n",
    "\n",
    "```\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Page Title</title>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<h6>This is a Heading</h6>\n",
    "<p>This is a paragraph.</p>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \"execute\" HTML directly in the cells of our Jupyter notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>This is a Heading</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check out the HTML source code of any website in our browser. This can be done by either right click anywhere on the website and select \"show source code\" (or similar) or use the shortcut **ctrl** + **u**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original website:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../misc/istari.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML source code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../misc/istari_source.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML consists of a series of elements which tell the browser how to display the content. An **HTML element** is defined by a **start tag** ```<tag>```, some **content** (e.g. text or a hyperlink), and an **end tag** ```</tag>```:\n",
    "\n",
    "```<tagname>Content goes here...</tagname>``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different html elements. Some of the most frequently used are:\n",
    "\n",
    "- **headings** are defined with the ```<h1>``` to ```<h6>``` tags\n",
    "- **paragraphs** are defined with the ```<p>``` tag\n",
    "- **links** are defined with the ```<a>``` tag\n",
    "- **images** are defined with the ```<img>``` tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Small Heading</h6>\n",
    "<p>This is a paragraph with a <a href=\"www.google.com\"> Link</a>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a Python package called *requests* to request and retrieve HTML from webpages. First, install requests using pip:\n",
    "\n",
    "```pip install requests```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installation (and restarting the Jupyter kernel), we can have to import the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can request HTML from any website using its **URL** (Uniform Resource Locator), colloquially termed a **web address**, and passing it to ```.get()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\"http://www.example.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns us ```<Response [200]>``` which is a ```Response``` object containing everything the server responded to our request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(requests.get(\"http://www.example.com\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```200``` is a HTML response code which stands for \"OK\" and it is the standard response for successful HTTP requests. Other important status codes are:\n",
    "\n",
    "- ```301``` Moved Permanently\n",
    "- ```403``` Forbidden\n",
    "- ```404``` Not Found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use ```.text``` on the response object to recieve the HTML code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://www.example.com\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautifulsoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to extract content from HTML is to use the Python package *beautifulsoup*. Beautiful Soup is a Python library for pulling data out of HTML files. So let's install and import it:\n",
    "\n",
    "```pip install beautifulsoup4```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to use BeautifulSoup to create a BeautifulSoup object, which represents the HTML document as a nested data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text)\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our BeautifulSoup object to directly retrieve elements from the HTML code. For example, ```.title``` extracts the title of the HTML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually returns a ```Tag``` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to get the content of the tag as a string, we just have to add a ```.string```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner, we can also access specific elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also handy functions included. ```.get_text()``` retrieves all strings from the HTML code. We can define a ```separator=\"\"``` to separate the invidiual contents and also tell BeautifulSoup to ```strip=True``` the content (removing trailing whitespaces and newline characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.get_text(separator=' ', strip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting all texts from a webpage boils down to a single line of Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup(requests.get(\"http://www.istari.ai/en\").text).get_text(separator=' ', strip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to find all the hyperlinks on a webpage, we can apply ```.find_all()``` on our BeautifulSoup object and pass the ```\"a\"``` tag. This will return us a list with ```<a>``` elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hyperlinks = BeautifulSoup(requests.get(\"http://www.istari.ai/en\").text).find_all(\"a\")\n",
    "all_hyperlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the actual hyperlinks, we have to apply ```.get(\"href\")``` on the individual ```tag``` objects. We can do so by iterating over the complete list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in all_hyperlinks:\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are quite a lot of duplicate links included. To get rid of them, the easiest way is to first extract the actual hyperlinks from the ```a``` elements and to put them into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hyperlinks = [link.get(\"href\") for link in all_hyperlinks]\n",
    "print(len(all_hyperlinks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then transfer this list to a ```set```. The items in a set are unordered, unchangeable, and do not allow duplicate values. This \"automatically\" deletes all duplicate entries in our list. We then just transfer our set back to a list using ```list()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hyperlink_list = list(set(hyperlink_list))\n",
    "print(len(unique_hyperlink_list))\n",
    "unique_hyperlink_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to extract specific elements from the html corpus, we can also use ```.find_all()``` for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup(requests.get(\"http://www.istari.ai/en\").text).find_all(\"div\", \"team-person-name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a simple scraper/crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue with building a simple crawler with the following functionalities:\n",
    "1. query websites using a random useragent header (string that lets servers and network peers identify the application, operating system, vendor, and/or version of the requesting user agent)\n",
    "2. extract texts from these websites\n",
    "3. extract hyperlinks from these websites\n",
    "4. follow one of these hyperlinks but make sure not query a visited website again\n",
    "5. repeat until a pre-defined number of websites were successfully scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user agent identifies us (or our web scraper) as a user to the server to which we send our request. It usually contains information about our operating system, resolution, browser and preferred language. Especially if we don't want to be recognized as a bot, it is recommended not to use the same user agent over and over again. For this we will use the Python package ```fake-useragent``` which you have to install first using pip.\n",
    "\n",
    "```pip install fake-useragent```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a random useragent (based on actual user statistics) is super easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserAgent().random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, every request should also go through a proxy that masks our actual IP. But for this you need a proxy provider, which forwards our requests in his \"name\" (IP) to the final destination. Assuming we had a proxy provider, we could simply enter their IP as a parameter \n",
    "\n",
    "```requests.get(url, proxies = { 'http': \"http://182.52.51.155:39236\", 'https': \"https://182.52.51.155:39236\"})```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to define a simple function that takes a url string as input and cleans it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(url):\n",
    "    url = \".\".join(url.split(\".\")[-2:]) # extract main domain\n",
    "    url = \"http://\" + url.replace(\"http://\", \"\").replace(\"https://\", \"\").replace(\"www.\", \"\") # make sure the format is \"http://example.com\"\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_url(\"www.cloud.istari.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function that extracts hyperlinks from BeautifulSoup objects and then extracts the associated domains, e.g. *istari.ai/products* should become *istari.ai*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse #urlparse extracts domains from urls\n",
    "\n",
    "def get_unqiue_domains_from_soup(soup):\n",
    "    all_hyperlinks = soup.find_all(\"a\") # get hyperlinks from soup\n",
    "    unique_domains = list(set([urlparse(link.get(\"href\")).netloc for link in all_hyperlinks])) # get unique domains from hyperlinks\n",
    "    unique_domains = [clean_url(domain) for domain in unique_domains if len(domain) > 0] # clean domains and filter empty results\n",
    "    \n",
    "    return unique_domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we also want to have a function that returns a timestamp string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def get_timestamp():\n",
    "    return datetime.datetime.fromtimestamp(time.time()).strftime('%d.%m.%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can build our simple webscraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "next_url = \"http://www.mannheim.de\"\n",
    "visited_domains = []\n",
    "unvisited_domains = []\n",
    "scraped_domains = []\n",
    "texts = []\n",
    "timestamps = []\n",
    "\n",
    "scraping_limit = 10\n",
    "\n",
    "\n",
    "while len(scraped_domains) < scraping_limit:\n",
    "    try:\n",
    "        # request page\n",
    "        response = requests.get(next_url, headers={'User-Agent': UserAgent().random})\n",
    "\n",
    "        # extract html and build soup\n",
    "        html = response.text\n",
    "        bs =  BeautifulSoup(html)\n",
    "\n",
    "        # get timestamp \n",
    "        timestamps.append(get_timestamp())\n",
    "\n",
    "        # extract text from html\n",
    "        text = bs.get_text(separator=' ', strip=True)\n",
    "        texts.append(text)\n",
    "\n",
    "        # get domain of response \n",
    "        domain = urlparse(response.url).netloc\n",
    "        visited_domains.append(clean_url(domain))\n",
    "        scraped_domains.append(clean_url(domain))\n",
    "\n",
    "        # get all unique domains, add previous unvisited dommains and filte visited ones\n",
    "        unique_domain_list = get_unqiue_domains_from_soup(bs) + unvisited_domains\n",
    "        unvisited_domains = [link for link in unique_domain_list if link not in visited_domains]\n",
    "\n",
    "        # select next website to visit at random\n",
    "        next_url = random.choice(unvisited_domains)\n",
    "\n",
    "        print(\"Sucessfully scraped:\", domain)\n",
    "        print(\"Now scraping:\", next_url)\n",
    "        print(\"########################\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Failed to scrape\", next_url)\n",
    "        visited_domains.append(next_url)\n",
    "        next_url = random.choice(unvisited_domains)\n",
    "        print(\"Now scraping\", next_url)\n",
    "        print(\"########################\")\n",
    "        \n",
    "print(\"Scraping limit reached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results can be transferred to a Pandas dataframe for further analysis (next week)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(list(zip(scraped_domains, timestamps, texts)), columns =[\"url\", \"timestamp\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
